{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Notices**\n",
    "\n",
    "Copyright (c) 2019 Intel Corporation.\n",
    "\n",
    "Permission is hereby granted, free of charge, to any person obtaining\n",
    "a copy of this software and associated documentation files (the\n",
    "\"Software\"), to deal in the Software without restriction, including\n",
    "without limitation the rights to use, copy, modify, merge, publish,\n",
    "distribute, sublicense, and/or sell copies of the Software, and to\n",
    "permit persons to whom the Software is furnished to do so, subject to\n",
    "the following conditions:\n",
    "\n",
    "The above copyright notice and this permission notice shall be\n",
    "included in all copies or substantial portions of the Software.\n",
    "\n",
    "THE SOFTWARE IS PROVIDED \"AS IS\", WITHOUT WARRANTY OF ANY KIND,\n",
    "EXPRESS OR IMPLIED, INCLUDING BUT NOT LIMITED TO THE WARRANTIES OF\n",
    "MERCHANTABILITY, FITNESS FOR A PARTICULAR PURPOSE AND\n",
    "NONINFRINGEMENT. IN NO EVENT SHALL THE AUTHORS OR COPYRIGHT HOLDERS BE\n",
    "LIABLE FOR ANY CLAIM, DAMAGES OR OTHER LIABILITY, WHETHER IN AN ACTION\n",
    "OF CONTRACT, TORT OR OTHERWISE, ARISING FROM, OUT OF OR IN CONNECTION\n",
    "WITH THE SOFTWARE OR THE USE OR OTHER DEALINGS IN THE SOFTWARE.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Image Processing and Data Augmentation Methods"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Objective\n",
    "\n",
    "Understand ways to find a data set and to prepare a data set for machine learning and training."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Activities \n",
    "**In this section of the training you will**\n",
    "- Image Preprocessing Techniques\n",
    "- Data Augmentation Techniques\n",
    "\n",
    "As you follow this notebook, complete **Activity** sections to finish this workload. \n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Find a Data set\n",
    "\n",
    "Artificial intelligence projects depend upon data. When beginning a project, data scientists look for existing data sets that are similar to or match the given problem. This saves time and money, and leverages the work of others, building upon the body of knowledge for all future projects. \n",
    "\n",
    "Typically you begin with a search engine query. For this project, we were looking for a data set with an unencumbered license.\n",
    "\n",
    "This project starts with [Vehicle Make and Model Recognition Dataset (VMMRdb)](http://vmmrdb.cecsresearch.org/)   which is large in scale and diversity, containing 9,170 classes consisting of 291,752 images, covering models manufactured between 1950 to 2016. VMMRdb dataset contains images that were taken by different users, different imaging devices, and multiple view angles, ensuring a wide range of variations to account for various scenarios that could be encountered in a real-life scenario. The cars are not well aligned, and some images contain irrelevant background. The data covers vehicles from 712 areas covering all 412 sub-domains corresponding to US metro areas. VMMRdb dataset can be used as a baseline for training a robust model in several real-life scenarios for traffic surveillance. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Part 1: Image Preprocessing\n",
    "### 1.1 Inspect Channels\n",
    "\n",
    "The images are usually stored in an RGB (Red Green Blue) format. In this format the image is represented as a three-dimensional (or three-channel) array. Now, we will take a look at three different channels individually for a random image. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "code_folding": []
   },
   "outputs": [],
   "source": [
    "#Inspect R, G, B Channels indivudally\n",
    "import glob, random\n",
    "import matplotlib.pyplot as plt\n",
    "import matplotlib.image as mpimg\n",
    "%matplotlib inline\n",
    "\n",
    "file_list = glob.glob(\"../Dataset/SubsetVMMR/*/*\")\n",
    "img_path = random.choice(file_list)\n",
    "image = mpimg.imread(img_path)\n",
    "fig, axes = plt.subplots(nrows=1, ncols=4, figsize = (15,15))\n",
    "\n",
    "#Original Image\n",
    "axes[0].set_title('Original Image')\n",
    "axes[0].imshow(image)\n",
    "# R channel\n",
    "axes[1].set_title('R channel')\n",
    "axes[1].imshow(image[:, :, 0])\n",
    "# G channel.\n",
    "axes[2].set_title('G channel')\n",
    "axes[2].imshow(image[:, :, 1])\n",
    "# B channel.\n",
    "axes[3].set_title('B channel')\n",
    "axes[3].imshow(image[:, :, 2])\n",
    "\n",
    "plt.setp([axes[1].get_yticklabels()], visible=False)\n",
    "plt.setp([axes[2].get_yticklabels()], visible=False)\n",
    "plt.setp([axes[3].get_yticklabels()], visible=False)\n",
    "fig.tight_layout()\n",
    "fig.subplots_adjust(top=0.88)\n",
    "\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 1.2 RGB-BGR Conversion\n",
    "\n",
    "Due to the underlying OpenCV implementation of imread, in some of the neural network topologies we need to convert an image from RGB to BGR format. \n",
    "\n",
    "**Activity**\n",
    "\n",
    "If RGB is represented (0,1,2), how to represent it in BGR? BGR=> (2,1,0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "code_folding": []
   },
   "outputs": [],
   "source": [
    "#Plot in RGB & BGR Mode\n",
    "fig, axes = plt.subplots(nrows=1, ncols=2, figsize = (15,15))\n",
    "\n",
    "#Image in RGB\n",
    "axes[0].set_title('RGB Mode')\n",
    "axes[0].imshow(image)\n",
    "#Image in BGR\n",
    "axes[1].set_title('BGR Mode')\n",
    "axes[1].imshow(image[:, :, (2,1,0)])\n",
    "\n",
    "\n",
    "plt.setp([axes[1].get_yticklabels()], visible=False)\n",
    "\n",
    "fig.tight_layout()\n",
    "fig.subplots_adjust(top=0.88)\n",
    "\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 1.3 Rescaling\n",
    "\n",
    "Rescaling is an operation that moves your data from one numerical range to another by simple division using a predefined constant. In deep neural networks you might want to restrict your input to the range from 0 to 1, due to possible overflow, optimization, stability issues, and so on.\n",
    "\n",
    "We use the Keras ImageDataGenerator  class, which allows us to do all transformations on the fly"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "code_folding": []
   },
   "outputs": [],
   "source": [
    "#Rescale Images\n",
    "import os\n",
    "from keras.preprocessing.image import ImageDataGenerator, array_to_img, img_to_array, load_img\n",
    "import numpy as np \n",
    "\n",
    "img_height, img_width = 299, 299\n",
    "train_path = \"../Dataset/\" + \"SubsetVMMR/\"\n",
    "def compare_images(img1, img2, Title1=\"Original\", Title2 =\"Transformed\", scale_option=False):\n",
    "    if type(img1) == np.ndarray:\n",
    "        img1 = array_to_img(img1, scale=scale_option)\n",
    "    if type(img2) == np.ndarray:\n",
    "        img2 = array_to_img(img2, scale=scale_option)\n",
    "    plt.figure(figsize=(14, 6))\n",
    "    plt.subplot(121)\n",
    "    plt.imshow(img1)\n",
    "    plt.axis(\"off\")\n",
    "    plt.title(Title1, fontsize=18)\n",
    "    plt.subplot(122)\n",
    "    plt.imshow(img2)\n",
    "    plt.axis(\"off\")\n",
    "    plt.title(Title2, fontsize=18)\n",
    "#Rescale colors\n",
    "datagen_rescaled = ImageDataGenerator(rescale=1. / 255.)\n",
    "datagen_default = ImageDataGenerator()\n",
    "\n",
    "gen_default = datagen_default.flow_from_directory(train_path, \n",
    "                                                  target_size=(img_height, img_width), \n",
    "                                                  batch_size=1, \n",
    "                                                  shuffle=False, \n",
    "                                                  class_mode=None)\n",
    "gen_rescaled = datagen_rescaled.flow_from_directory(train_path, \n",
    "                                                    target_size=(img_height, img_width), \n",
    "                                                    batch_size=1, \n",
    "                                                    shuffle=False, \n",
    "                                                    class_mode=None)\n",
    "\n",
    "sample_default = next(gen_default)\n",
    "sample_rescaled = next(gen_rescaled)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "By rescaling an image, we are dividing each pixel with 255 and having a look at the transformed array representations helps you understand your transformations. The **scale_option** parameter helps you understand the image transformation better. It's default value is set to **False** and hence you can see the difference. If you change it **True**, you will see that two images are the same since **array_to_img** module rescales it back to original image. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Plot images to make comparison\n",
    "compare_images(sample_default[0], sample_rescaled[0], scale_option = True)\n",
    "#Check the Arrays to see the difference\n",
    "[sample_default[0][:2, :2, 0], sample_rescaled[0][:2, :2, 0]]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 1.4 Grayscaling\n",
    "\n",
    "Grayscaling turns a color RGB image into images with only shades of gray representing colors. This pair of transformations can throw away noisy pixels and detect shapes in the picture. Nowadays, all these operations are learned through convolutional neural networks. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "code_folding": []
   },
   "outputs": [],
   "source": [
    "#Image Grayscaling\n",
    "gen_grayscaled = datagen_default.flow_from_directory(train_path, \n",
    "                                                     target_size=(img_height, img_width), \n",
    "                                                     batch_size=1, \n",
    "                                                     shuffle=False, \n",
    "                                                     class_mode=None, \n",
    "                                                     color_mode=\"grayscale\")\n",
    "sample_grayscaled = next(gen_grayscaled)\n",
    "compare_images(sample_default[0], sample_grayscaled[0])\n",
    "#Check the Arrays to see the difference\n",
    "[sample_default[0][:2, :2, 0], sample_grayscaled[0][:2, :2, 0]]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 1.5 Samplewise centering (across features inside one sample)\n",
    "\n",
    "We’ve already seen that raw data values are from 0 to 255. So, one sample is a 3D array of numbers from 0 to 255. Following the optimization stability considerations (get rid of vanishing or saturating values problems) we might want to normalize the dataset such that the mean value of each data sample would be equal to 0.\n",
    "\n",
    "By looking at the the following values we can understand it a better:\n",
    "\n",
    "np.mean(sample_samplewise_mean[0][:,:,i]) where i = 0,1,2 and overall sum would be equal to zero. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "code_folding": [],
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#Samplewise centering \n",
    "datagen_samplewise_mean = ImageDataGenerator(samplewise_center=True)\n",
    "gen_samplewise_mean = datagen_samplewise_mean.flow_from_directory(train_path, \n",
    "                                                                  target_size=(img_height, img_width), \n",
    "                                                                  batch_size=1, \n",
    "                                                                  shuffle=False,  \n",
    "                                                                  class_mode=None)\n",
    "sample_samplewise_mean = next(gen_samplewise_mean)\n",
    "compare_images(sample_default[0], sample_samplewise_mean[0])\n",
    "#Check the Arrays to see the difference\n",
    "[sample_default[0][:2, :2, 0], sample_samplewise_mean[0][:2, :2, 0]]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 1.6 Samplewise std normalization (across features inside one sample)\n",
    "\n",
    "This preprocessing step follows the same idea as samplewise centering, but instead of setting the mean value to 0, it sets the standard deviation value to 1.\n",
    "\n",
    "**Activity**\n",
    "\n",
    "Set **samplewise_std_normalization** to True to create your generator. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "code_folding": []
   },
   "outputs": [],
   "source": [
    "#Sampliwse std normalization\n",
    "datagen_samplewise_std = ImageDataGenerator(samplewise_std_normalization=True)\n",
    "gen_samplewise_std = datagen_samplewise_std.flow_from_directory(train_path, \n",
    "                                                                target_size=(img_height, img_width), \n",
    "                                                                batch_size=1, \n",
    "                                                                shuffle=False, \n",
    "                                                                class_mode=None)\n",
    "sample_samplewise_std = next(gen_samplewise_std)\n",
    "compare_images(sample_default[0], sample_samplewise_std[0])\n",
    "#Check the Arrays to see the difference\n",
    "[sample_default[0][:2, :2, 0], sample_samplewise_std[0][:2, :2, 0]]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Part 2: Data Augmentation Techniques\n",
    "\n",
    "In this section, we’re going to look at more data-dependent transformations, which explicitly use the graphical nature of data. These kinds of transformations are often used for data augmentation procedures."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2.1 Rotation\n",
    "\n",
    "This transformation rotates the image in a certain direction (clockwise or counterclockwise).\n",
    "\n",
    "**Activity**\n",
    "\n",
    "Change **rotation_range** to a degree of your choice so that the image will be rotated with that degree in clockwise direction. Example: rotation_range = 45"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "code_folding": [
     2
    ]
   },
   "outputs": [],
   "source": [
    "#Image Rotation\n",
    "datagen_rotated = ImageDataGenerator(rotation_range=45, fill_mode=\"constant\")\n",
    "gen_rotated = datagen_rotated.flow_from_directory(train_path, \n",
    "                                                  target_size=(img_height, img_width), \n",
    "                                                  batch_size=1, \n",
    "                                                  shuffle=False, \n",
    "                                                  class_mode=None)\n",
    "sample_rotated = next(gen_rotated)\n",
    "compare_images(sample_default[0], sample_rotated[0])\n",
    "#Check the Arrays to see the difference\n",
    "[sample_default[0][:2, :2, 0], sample_rotated[0][:2, :2, 0]]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2.2 Horizontal & Vertical Shift\n",
    "\n",
    "This technique can shift an image horizontally or vertically."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "code_folding": [
     4,
     9
    ]
   },
   "outputs": [],
   "source": [
    "#Horizontal and Vertical Shift\n",
    "datagen_hshifted = ImageDataGenerator(width_shift_range=0.4, fill_mode=\"constant\")\n",
    "datagen_vshifted = ImageDataGenerator(height_shift_range=0.4, fill_mode=\"constant\")\n",
    "\n",
    "gen_hshifted = datagen_hshifted.flow_from_directory(train_path, \n",
    "                                                    target_size=(img_height, img_width), \n",
    "                                                    batch_size=1, \n",
    "                                                    shuffle=False, \n",
    "                                                    class_mode=None)\n",
    "gen_vshifted = datagen_vshifted.flow_from_directory(train_path, \n",
    "                                                    target_size=(img_height, img_width), \n",
    "                                                    batch_size=1, \n",
    "                                                    shuffle=False, \n",
    "                                                    class_mode=None)\n",
    "sample_hshifted = next(gen_hshifted)\n",
    "sample_vshifted = next(gen_vshifted)\n",
    "compare_images(sample_hshifted[0], sample_vshifted[0], Title1=\"Horizontal Shifted\", Title2=\"Vertical Shifted\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2.3 Horizontal & Vertical Flip\n",
    "\n",
    "With this method, we can flip an image both ways.\n",
    "\n",
    "**Activity** \n",
    "\n",
    "Create generators. Example: \n",
    "\n",
    "datagen_hflipped = ImageDataGenerator(horizontal_flip=True)\n",
    "\n",
    "datagen_vflipped = ImageDataGenerator(vertical_flip=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "code_folding": [
     4,
     9
    ],
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#Horizontal and Vertical Flip\n",
    "datagen_hflipped = ImageDataGenerator(horizontal_flip=True)\n",
    "datagen_vflipped = ImageDataGenerator(vertical_flip=True)\n",
    "\n",
    "gen_hflipped = datagen_hflipped.flow_from_directory(train_path, \n",
    "                                                    target_size=(img_height, img_width), \n",
    "                                                    batch_size=1, \n",
    "                                                    shuffle=False, \n",
    "                                                    class_mode=None)\n",
    "gen_vflipped = datagen_vflipped.flow_from_directory(train_path, \n",
    "                                                    target_size=(img_height, img_width), \n",
    "                                                    batch_size=1, \n",
    "                                                    shuffle=False, \n",
    "                                                    class_mode=None)\n",
    "sample_hflipped = next(gen_hflipped)\n",
    "sample_vflipped = next(gen_vflipped)\n",
    "compare_images(sample_hflipped[0], sample_vflipped[0], Title1=\"Horizontal Flip\", Title2=\"Vertical Flip\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2.4 Zoom & Shearing\n",
    "\n",
    "You can zoom in an image or do shearing which displaces each point in a direction by an amount proportional to its distance from an edge of the image."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "code_folding": [],
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#Zoom in and Shearing \n",
    "datagen_sheared = ImageDataGenerator(shear_range=0.75, fill_mode=\"constant\")\n",
    "datagen_zoomed = ImageDataGenerator(zoom_range=0.5, fill_mode=\"constant\")\n",
    "\n",
    "gen_sheared = datagen_sheared.flow_from_directory(train_path, \n",
    "                                                  target_size=(img_height, img_width), \n",
    "                                                  batch_size=1, \n",
    "                                                  shuffle=False, \n",
    "                                                  class_mode=None)\n",
    "gen_zoomed = datagen_zoomed.flow_from_directory(train_path, \n",
    "                                                target_size=(img_height, img_width), \n",
    "                                                batch_size=1, \n",
    "                                                shuffle=False, \n",
    "                                                class_mode=None)\n",
    "sample_sheared = next(gen_sheared)\n",
    "sample_zoomed = next(gen_zoomed)\n",
    "compare_images(sample_zoomed[0], sample_sheared[0], Title1=\"Zoom In\", Title2=\"Shearing\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2.5 Combination\n",
    "\n",
    "It's possible to apply all these image transformation techniques at once and create new data samples. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "code_folding": [
     1
    ]
   },
   "outputs": [],
   "source": [
    "#Select a random image and follow the next step\n",
    "datagen = ImageDataGenerator(rotation_range=45, \n",
    "                             width_shift_range=0.2, \n",
    "                             height_shift_range=0.2, \n",
    "                             shear_range=0.2, \n",
    "                             zoom_range=0.3, \n",
    "                             horizontal_flip=True, \n",
    "                             vertical_flip=True, \n",
    "                             fill_mode=\"nearest\")\n",
    "#Load example image\n",
    "img = load_img(img_path)\n",
    "print(img_path.split(\"/\")[1])\n",
    "img"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "code_folding": []
   },
   "outputs": [],
   "source": [
    "#Apply different augmentation techniques\n",
    "img = img_to_array(img)\n",
    "img = img.reshape((1,) + img.shape)\n",
    "n_augmentations = 4\n",
    "plt.figure(figsize=(15, 6))    \n",
    "i = 0\n",
    "\n",
    "for batch in datagen.flow(img, \n",
    "                          batch_size=1, \n",
    "                          seed=21):\n",
    "    \n",
    "    plt.subplot(2, int(np.ceil(n_augmentations * 1. / 2)), i + 1)\n",
    "    plt.imshow(array_to_img(batch[0]))\n",
    "    plt.axis(\"off\")\n",
    "    \n",
    "    i += 1\n",
    "    if i >= n_augmentations:\n",
    "        break"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Resources\n",
    "\n",
    "TensorFlow* Optimizations on Modern Intel® Architecture, https://software.intel.com/en-us/articles/tensorflow-optimizations-on-modern-intel-architecture\n",
    "\n",
    "Intel Optimized TensorFlow Wheel Now Available, https://software.intel.com/en-us/articles/intel-optimized-tensorflow-wheel-now-available\n",
    "\n",
    "Build and Install TensorFlow* on Intel® Architecture, https://software.intel.com/en-us/articles/build-and-install-tensorflow-on-intel-architecture\n",
    "\n",
    "TensorFlow, https://www.tensorflow.org/"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Case Studies\n",
    "\n",
    "Manufacturing Package Fault Detection Using Deep Learning, https://software.intel.com/en-us/articles/manufacturing-package-fault-detection-using-deep-learning\n",
    "\n",
    "Automatic Defect Inspection Using Deep Learning for Solar Farm, https://software.intel.com/en-us/articles/automatic-defect-inspection-using-deep-learning-for-solar-farm"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Citations\n",
    "\n",
    "A Large and Diverse Dataset for Improved Vehicle Make and Model Recognition\n",
    "F. Tafazzoli, K. Nishiyama and H. Frigui\n",
    "In Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition (CVPR) Workshops 2017. \n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
